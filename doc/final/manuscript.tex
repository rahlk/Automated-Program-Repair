\documentclass[]{sig-alternate}
%\usepackage{stfloats}
\usepackage{tikz}
\def\firstcircle{(90:1.75cm) circle (2.5cm)}
\def\secondcircle{(210:1.75cm) circle (2.5cm)}
\def\thirdcircle{(330:1.75cm) circle (2.5cm)}

\usepackage[framed]{ntheorem}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shadows}
%\newtheorem{Lesson}{Lesson}
\theoremclass{Lesson}
\theoremstyle{break}

% inner sep=10pt,
\tikzstyle{thmbox} = [rectangle, rounded corners, draw=black,
fill=Gray!20,  drop shadow={fill=black, opacity=1}]
\newcommand\thmbox[1]{%
	\noindent\begin{tikzpicture}%
	\node [thmbox] (box){%
		\begin{minipage}{.94\textwidth}%
		\vspace{-3mm}#1\vspace{-3mm}%
		\end{minipage}%
	};%
	\end{tikzpicture}}

\let\theoremframecommand\thmbox
\newshadedtheorem{lesson}{Research answer}


\usepackage{booktabs}
\usepackage{comment}
\usepackage{cite}
\usepackage{framed,graphicx,xcolor}
\usepackage{multirow}
\usepackage{rotating}
%\usepackage{stfloats}
\usepackage[shortlabels]{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{balance}
\usepackage{bigstrut}
\newcommand{\bi}{\begin{itemize}}
	\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
	\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\textsection\ref{sect:#1}}
\newcommand{\tab}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
%\setlist{nolistsep,leftmargin=5mm}
%\usepackage[pdftex]{graphicx}
% \usepackage{program}
\newcommand{\Sample}{{\bf SAMPLE}}
\newcommand{\PEEKING}{{\bf PEEKING2}}
%\usepackage[table]{xcolor}
\definecolor{darkgreen}{rgb}{0,0.3,0}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\usepackage{colortbl}
\usepackage{picture}
\usepackage{url}
\usepackage{hyphenat}
% \usepackage{hyperref}
%\usepackage{listings}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}

\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Comments}{rgb}{0.5,0.5,0.5}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{1,1,1}
\definecolor{lavenderpink}{rgb}{0.98, 0.68, 0.82}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}

\newcommand{\quart}[4]{\begin{picture}(80,4)%1
	{\color{black}\put(#3,2){\circle*{4}}\put(#1,2){\line(1,0){#2}}}\end{picture}}
\usepackage{url}

\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45}
\newenvironment{changed}{\par\color{MyDarkBlue}}{\par}
%\newenvironment{changed}{\par}{\par}

\newcommand{\ADD}[1]{\textcolor{MyDarkBlue}{{\bf #1}}}
\usepackage{times}
% \pagenumbering{arabic}
\usepackage[splitrule]{footmisc} %% The splitrule option draws a full width
%%rule above the continued part of the footnote as a visual cue to readers.
\usepackage{balance}
% \usepackage{flushend}
\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000


\begin{document}

     \definecolor{shadecolor}{gray}{0.9}
     \conferenceinfo{CSC791:}{Automated Program Repair}
     \title{Too Much Automation? The Bellwether Effect and Its Implications for Transfer Learning}
     \numberofauthors{1}
     \author{
     	\alignauthor
     	Rahul~Krishna \hspace{40pt} Anwesha~Das\\
     	\affaddr{Computer Science, North Carolina State University, USA}\\
     	{\{rkrish11, adas4\}@ncsu.edu}}
     \maketitle

		 \begin{abstract}

			 Automated Patch generation has garnered a lot of attention in the past decade. It has long been a elusive goal in software engineering. Automation has been a key research area and yet debugging/patch generation remains a largely manual process. One success story in this endeavor is \textit{GenProg} . In fact, GenProg, which is widely accepted as a promising tool by the research community, is now used as a baseline to compare other new techniques. Albeit popular, there is a general lack of consensus on the need for such complex tools to generate effective patches. While some researchers~\cite{leGoues12} opine that GenProg is quite effective in generating patches, given the existence of test cases, others \cite{qi2014, kim2013, arcuri2011} disagree. They say that it is quite unclear if the the efficacy of GenProg is merely due to its ``mutate'' operator or due to genetic programming.\\

			 \noindent{\bf Categories/Subject Descriptors:} D.2 [Software Engineering]; I.2
        [Artificial Intelligence];

		 \keywords{Defect Prediction; Data Mining; Transfer learning}
		 \end{abstract}

\section{Introduction}


This issue was examined by Qi et at.~\cite{qi2014}. With their tool RSRepair (\underline{\textit{R}}andom-\underline{\textit{S}}earch \underline{\textit{Repair}}), they demonstrated the benefit of random search over GenProg.

It is my hypothesis that the lack of efficiency of GenProg in this area is not due to the failure of Genetic Algorithms (or any evolutionary computation scheme for that matter), but due to incorrect and perhaps even incomplete modeling of the search space.

As a motivating example consider Genetic Programming. The algorithm works by trying to maximize the ``fitness'' score. In this case, the number of passing test cases given a patch. RSRepair, on the other hand, considers either repair effectiveness (as measured by the number patch trials required) or repair efficiency (measured by the number of test case executions). It not hard to see that each of these objectives namely, the number of passing test cases, the repair effectiveness, and the repair efficiency, are all important. Perhaps some are more important than others, but they all play a crucial role nonetheless.

Therefore, I conjecture that the research in the area of automated patch generation will be best guided by modeling the domain as a multiobjective search problem with the aforementioned objectives. This, in my opinion, will allow us to leverage the wealth of research in Search Based Software Engineering (referred to henceforth as \textit{SBSE}). To this end, this project asks the following two research questions:

\subsubsection*{RQ1: Can automated patch generation be modeled as a multiobjective search problem?}
Although it makes theoretical sense to view patch generation as containing multiple objectives, this question asks if it is practical to model it as such.

\subsubsection*{RQ2: If yes, what is the best multiobjective search to generate patches automatically?}
\label{rq2}
If the answer to RQ1 is affirmative. This question aims to explore various multiobjective optimizers to identify the best algorithm. Specifically, this project will compare three standard SBSE techniques: NSGAII, MOEAD, Genetic Programming, using Random Search as a baseline. In addition to this, the usefulness of a promising new algorithm called SWAY\footnote{This algorithm is in current development by our research group}~\cite{chen2016sampling, nair2016accidental} will be also be investigated.

\section{An Overview of Patch Generation}

Generally, when a bug is reported, an automated program repair scheme undertakes the following three steps:

\begin{enumerate}
    \item \textit{Fault localization:}
    These techniques are used to identify suspicious faulty code snippet that may a have caused the bug.

    \item \textit{Patch Generation:}
    Once faulty code snippet has been located, many candidate patches can be generated through the modifications to that code snippet, according to specific repair rules based on either evolutionary computation or code-based contracts.

    \item \textit{Patch Validation:}
    When a candidate patch has been produced, regression testing, inclusive of negative test cases (reproducing the fault) and positive test cases (characterizing the normal behaviors), is commonly used to validate the correctness of produced candidate patch.

\end{enumerate}

The above procedure can be iterated over and over again until some valid patch is found. Any patch passing all these test cases is considered valid.

\input{aux_tex/measures.tex}
\section{A (very) Brief Overview of Multiobjective Search}

A search is the task of finding one or more solutions which satisfy one or more specified objectives. While a single-objective optimization involves a single objective function and a single solutions, a multiobjective optimization considers several objectives simultaneously. In reference to the current problem we have three competing objectives, all of which need to be maximized: (1) Number of passing test cases; (2) The repair effectiveness; and (3) The repair efficiency

In such a case, a multiobjective optimizer generates a set of alternate solution with certain trade-offs. These are called \textit{Pareto Optimal Solutions}. The solutions, in our case, will be a set of valid patches that perform equally well when measured in terms of the three objectives.

Multiobjective problems are usually complex, NP-Hard, and resource intensive. Although exact methods can be used, they consume prohibitively large amounts of time and memory. An alternative approach would be to make use of metaheuristic algorithms from RQ2. These approximate the Pareto frontier in a reasonable amount of time.

\section{Evaluation}

%\input{aux_tex/datasets.tex}

\subsection{Metrics}
Figure \ref{fig:measure} highlights some of the preliminary performance evaluation measures we use to evaluate the algorithms. Please note that this will be expanded in future to accommodate other measures.

\subsubsection{How to calculate them?}
Calculating these metrics are well studied~\cite{deb2002fast, goel2010study}. We have an implementation in place that can readily give these metrics, given a set of ideal solutions (in this case, valid patches).

\subsection{Datasets}
The data set required for evaluating will be obtained from some recent work on GenProg~\cite{leGoues12}. My reading of the literature suggests that these dataset are the ``benchmarks'', used by several researchers~\cite{qi2014}. These are C programs with all the bugs reported reported in the historical versions as tabulated in Figure~\ref{fig:datasets}.

\subsection{Tools}
As for tools, it is worth noting that AST can be generated from C programs using a tool called CIL\footnote{https://sourceforge.net/projects/cil/}.

\subsection{Analysis Plan}
To provide a more comprehensive comparison, the optimizers will be ranked ranked statistically. To do this, I shall use the Scott-Knott procedure recommended by Mittas \& Angelis\cite{mittas13}.

This method sorts a list of $l$ treatments with $ls$ measurements by their median
score. It then
splits $l$ into sub-lists $m,n$ in order to maximize the expected value of
differences  in the observed performances
before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$:
\[E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2\]
Scott-Knott then applies some statistical hypothesis test $H$ to check
if $m,n$ are significantly different. If so, Scott-Knott then recurses on each division.



\section{Proposed Timeline}

Figure \ref{time} highlights the proposed timeline for the tasks undertaken in the project. Please note again that this subject to change, please see https://github.com/ai-se/spatch/milestones for the latest milestone progress.\\
\pagebreak
\bibliographystyle{unsrt}
\bibliography{References}
\end{document}
